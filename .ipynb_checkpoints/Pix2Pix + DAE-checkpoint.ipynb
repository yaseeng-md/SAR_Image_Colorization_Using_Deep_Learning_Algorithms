{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb5de2f-06bf-4a8b-b24d-9ec3e81907c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from skimage.metrics import structural_similarity\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.models import vgg16, vgg19\n",
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n",
    "\n",
    "from Create_Dataset import PairedImageDataset\n",
    "from Pairing_Images import PairFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83822aee-53a6-41ca-9659-8a5c09ae2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (256,256,3)\n",
    "TARGET_SHAPE = (256,256,3)\n",
    "BATCH_SIZE = 1\n",
    "# Dataset Hyper Parameters\n",
    "subset = \"agri\"\n",
    "save_dataframe = \"True\"\n",
    "s1_image_path = \"Dataset/agri/s1/\"\n",
    "s2_image_path = \"Dataset/agri/s2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c477ffe-65c5-46fc-ae0f-ed22c82d7d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Instances = 4000\n",
      "Sample Instances = 1000\n",
      "To plot Instances = 10\n"
     ]
    }
   ],
   "source": [
    "image_dataset = PairedImageDataset(s1_dir=s1_image_path, s2_dir=s2_image_path, subset_name=subset, save_dataframe=save_dataframe)\n",
    "dataloader = DataLoader(image_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f\"Total Instances = {len(image_dataset)}\")\n",
    "\n",
    "subset_indices = list(range(min(1000, len(image_dataset))))\n",
    "subset_dataset = Subset(image_dataset, subset_indices)\n",
    "subset_loader = DataLoader(subset_dataset, batch_size=1, shuffle=False) \n",
    "print(f\"Sample Instances = {len(subset_dataset)}\")\n",
    "\n",
    "\n",
    "plot_indices = list(range(10))\n",
    "plot_dataset = Subset(image_dataset, plot_indices)\n",
    "plot_loader = DataLoader(plot_dataset,batch_size = 1, shuffle = False)\n",
    "print(f\"To plot Instances = {len(plot_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73d1b801-833f-4dad-a93e-409663e4810f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256]) torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i,j in subset_dataset:\n",
    "    print(i.shape,j.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16d530d0-5342-48a8-92e5-9d0162749e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetBlockDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, apply_batchnorm=True):\n",
    "        super(UNetBlockDown, self).__init__()\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "        if apply_batchnorm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class UNetBlockUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, apply_dropout=False):\n",
    "        super(UNetBlockUp, self).__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        if apply_dropout:\n",
    "            layers.append(nn.Dropout(0.5))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.block(x)\n",
    "        x = torch.cat([x, skip_input], dim=1)  \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=3):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "        self.down1 = UNetBlockDown(in_channels, 64, apply_batchnorm=False)\n",
    "        self.down2 = UNetBlockDown(64, 128)\n",
    "        self.down3 = UNetBlockDown(128, 256)\n",
    "        self.down4 = UNetBlockDown(256, 512)\n",
    "        self.down5 = UNetBlockDown(512, 512)\n",
    "        self.down6 = UNetBlockDown(512, 512)\n",
    "        self.down7 = UNetBlockDown(512, 512)\n",
    "        self.down8 = UNetBlockDown(512, 512, apply_batchnorm=False) \n",
    "\n",
    "        self.up1 = UNetBlockUp(512, 512, apply_dropout=True)\n",
    "        self.up2 = UNetBlockUp(1024, 512, apply_dropout=True)\n",
    "        self.up3 = UNetBlockUp(1024, 512, apply_dropout=True)\n",
    "        self.up4 = UNetBlockUp(1024, 512)\n",
    "        self.up5 = UNetBlockUp(1024, 256)\n",
    "        self.up6 = UNetBlockUp(512, 128)\n",
    "        self.up7 = UNetBlockUp(256, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "        return self.final(u7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d542c24b-5528-42a9-afb5-5af4c084bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            self._block(in_channels * 2, 64, norm=False),\n",
    "            self._block(64, 128),\n",
    "            self._block(128, 256),\n",
    "            nn.ZeroPad2d(1),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.ZeroPad2d(1),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, norm=True):\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "        if norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input_image, target_image):\n",
    "        x = torch.cat([input_image, target_image], dim=1) \n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17bc8ecd-dd48-4548-921c-aacba97b2492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "\n",
    "        # ----------> Encoder <----------\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=5, padding=2),  # (B, 64, H, W)\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # (B, 64, H/2, W/2)\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),          # (B, 128, H/2, W/2)\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # (B, 128, H/4, W/4)\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),         # (B, 256, H/4, W/4)\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)                  # (B, 256, H/8, W/8)\n",
    "        )\n",
    "\n",
    "        # ----------> Decoder <----------\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 256, kernel_size=3, padding=1)  # (B, 256, H/8, W/8)\n",
    "        self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)  # (B, 128, H/4, W/4)\n",
    "        self.deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)   # (B, 64, H/2, W/2)\n",
    "        self.deconv4 = nn.ConvTranspose2d(64, out_channels, kernel_size=5, padding=2)  # (B, 3, H, W)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.upsample = lambda x: F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.final_activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.relu(self.deconv1(x))\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        x = self.relu(self.deconv2(x))\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        x = self.relu(self.deconv3(x))\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        x = self.deconv4(x)\n",
    "        x = self.final_activation(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b808350-0f9c-44b6-8053-a0856b983fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = UNetGenerator(in_channels=3, out_channels=3).to(device)\n",
    "dae = DenoisingAutoencoder(in_channels=3).to(device)\n",
    "discriminator = PatchDiscriminator(in_channels=3).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b13d7-a1db-4334-977d-731c84098ea2",
   "metadata": {},
   "source": [
    "#### Stacking UNet Generaotor + AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e83145e7-42db-4d5f-abb7-0c14756153fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationWithDenoising(nn.Module):\n",
    "    def __init__(self, generator, dae):\n",
    "        super(ColorizationWithDenoising, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.dae = dae\n",
    "\n",
    "    def forward(self, grayscale_input):\n",
    "        # grayscale_input: [B, 1, 256, 256]\n",
    "        colorized = self.generator(grayscale_input)         # [B, 3, 256, 256], range [-1, 1]\n",
    "        denoised = self.dae(colorized)                      # [B, 3, 256, 256], range [-1, 1]\n",
    "        return denoised, colorized  # returning both for loss comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c99443fb-3699-4ad1-9d51-7c6666449a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(generated_image, target_image):\n",
    "    \"\"\"\n",
    "    Compute SSIM, PSNR, MSE, RMSE between generated and target images.\n",
    "    Both inputs are PyTorch tensors in the range [-1, 1].\n",
    "    \"\"\"\n",
    "    # Move to CPU, detach, and convert to numpy\n",
    "    generated_image = generated_image.squeeze().detach().cpu().numpy()\n",
    "    target_image = target_image.squeeze().detach().cpu().numpy()\n",
    "\n",
    "    # Rescale from [-1, 1] to [0, 1]\n",
    "    generated_image = (generated_image + 1) / 2.0\n",
    "    target_image = (target_image + 1) / 2.0\n",
    "\n",
    "    # Transpose from (C, H, W) to (H, W, C)\n",
    "    generated_image = np.transpose(generated_image, (1, 2, 0))\n",
    "    target_image = np.transpose(target_image, (1, 2, 0))\n",
    "\n",
    "    # Clip values\n",
    "    generated_image = np.clip(generated_image, 0, 1)\n",
    "    target_image = np.clip(target_image, 0, 1)\n",
    "\n",
    "    # Determine win_size\n",
    "    min_height = min(generated_image.shape[0], target_image.shape[0])\n",
    "    min_width = min(generated_image.shape[1], target_image.shape[1])\n",
    "    win_size = min(min_height, min_width)\n",
    "    win_size = win_size if win_size % 2 == 1 else win_size - 1  # make it odd\n",
    "\n",
    "    # Ensure win_size is at least 3\n",
    "    win_size = max(3, win_size)\n",
    "\n",
    "    # Compute metrics\n",
    "    ssim = structural_similarity(\n",
    "        target_image, generated_image,\n",
    "        channel_axis=-1, data_range=1.0,\n",
    "        win_size=win_size\n",
    "    )\n",
    "    psnr = peak_signal_noise_ratio(target_image, generated_image, data_range=1.0)\n",
    "    mse = mean_squared_error(target_image.flatten(), generated_image.flatten())\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return {\n",
    "        \"SSIM\": ssim,\n",
    "        \"PSNR\": psnr,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse\n",
    "    }\n",
    "\n",
    "def evaluate_model(test_loader, generator, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set using image quality metrics.\n",
    "    \"\"\"\n",
    "    ssim_scores, psnr_scores, mse_scores, rmse_scores = [], [], [], []\n",
    "\n",
    "    generator.to(device)\n",
    "    generator.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_image, target_image in test_loader:\n",
    "            input_image = input_image.to(device)\n",
    "            target_image = target_image.to(device)\n",
    "\n",
    "            # Ensure input is 4D (B, C, H, W)\n",
    "            if input_image.dim() == 3:\n",
    "                input_image = input_image.unsqueeze(0)\n",
    "                target_image = target_image.unsqueeze(0)\n",
    "\n",
    "            generated_image = generator(input_image)\n",
    "\n",
    "            metrics = compute_metrics(generated_image, target_image)\n",
    "            ssim_scores.append(metrics[\"SSIM\"])\n",
    "            psnr_scores.append(metrics[\"PSNR\"])\n",
    "            mse_scores.append(metrics[\"MSE\"])\n",
    "            rmse_scores.append(metrics[\"RMSE\"])\n",
    "\n",
    "    # Print averaged results\n",
    "    print(f\"SSIM: {np.mean(ssim_scores):.4f} | PSNR: {np.mean(psnr_scores):.2f} dB | MSE: {np.mean(mse_scores):.4f} | RMSE: {np.mean(rmse_scores):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb177a70-54e3-463c-8aed-5a6651e321ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA_L1 = 100\n",
    "LAMBDA_PERC = 1\n",
    "loss_object = nn.BCEWithLogitsLoss()\n",
    "l1_loss_fn = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58fd717c-0a63-46e0-a417-65ef8b6515c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_colorization_results(generator, subset_loader, num_images=10):\n",
    "    \"\"\"\n",
    "    Visualizes grayscale input, ground truth color, and generated colorized images.\n",
    "\n",
    "    Args:\n",
    "        generator (torch.nn.Module): Trained generator model for colorization.\n",
    "        subset_loader (DataLoader): DataLoader providing grayscale and color image pairs.\n",
    "        num_images (int): Number of images to visualize (default is 10).\n",
    "    \"\"\"\n",
    "    # Set generator to evaluation mode\n",
    "    generator.eval()\n",
    "\n",
    "    # Automatically get device of the generator\n",
    "    device = next(generator.parameters()).device\n",
    "\n",
    "    # Counter for number of images plotted\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through the dataset\n",
    "    for gray, color in subset_loader:\n",
    "        # Move input to same device as model\n",
    "        gray = gray.to(device)\n",
    "\n",
    "        # Generate image\n",
    "        with torch.no_grad():\n",
    "            generated = generator(gray)\n",
    "\n",
    "        # Move tensors to CPU for visualization\n",
    "        gray_np = gray[0].cpu().permute(1, 2, 0).numpy()\n",
    "        color_np = color[0].cpu().permute(1, 2, 0).numpy()\n",
    "        gen_np   = generated[0].cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Scale from [-1, 1] to [0, 1] if needed\n",
    "        gray_np = (gray_np + 1) / 2.0\n",
    "        color_np = (color_np + 1) / 2.0\n",
    "        gen_np   = (gen_np + 1) / 2.0\n",
    "\n",
    "        # Plot images\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        titles = ['Gray Scale Input', 'Ground Truth Color', 'Generated Image']\n",
    "        images = [gray_np, color_np, gen_np]\n",
    "\n",
    "        for i in range(3):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            plt.imshow(images[i])\n",
    "            plt.title(titles[i])\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        count += 1\n",
    "        if count >= num_images:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcaa37b5-9f39-4810-82e3-1dd56c1454ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gandl\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gandl\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# ------------------> Build VGG Feature Extractor <------------------\n",
    "class VGG19FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19FeatureExtractor, self).__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features\n",
    "        self.feature_extractor = nn.Sequential(*list(vgg.children())[:12])  # Up to relu3_3\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feature_extractor(x)\n",
    "\n",
    "# ------------------> Preprocess Function for VGG <------------------\n",
    "def preprocess(img_tensor):\n",
    "    \"\"\"\n",
    "    img_tensor: (B, 3, H, W), in [-1, 1]\n",
    "    Output: normalized for VGG (mean-subtracted and scaled)\n",
    "    \"\"\"\n",
    "    # Convert [-1, 1] to [0, 1]\n",
    "    img_tensor = (img_tensor + 1) / 2.0\n",
    "    normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    return normalize(img_tensor)\n",
    "\n",
    "# ------------------> Perceptual Loss Class <------------------\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.vgg = VGG19FeatureExtractor().to(device)\n",
    "        self.l1 = nn.L1Loss()\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "        # Preprocess and move to same device as VGG\n",
    "        y_true = preprocess(y_true).to(next(self.vgg.parameters()).device)\n",
    "        y_pred = preprocess(y_pred).to(next(self.vgg.parameters()).device)\n",
    "\n",
    "        # Extract VGG features\n",
    "        features_true = self.vgg(y_true)\n",
    "        features_pred = self.vgg(y_pred)\n",
    "\n",
    "        # Compute L1 loss between features\n",
    "        return self.l1(features_true, features_pred)\n",
    "\n",
    "# ------------------> Instantiate on the Correct Device <------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "perceptual_loss = PerceptualLoss(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73b8fbac-8b60-4e73-813c-fafadd6ccad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(disc_generated_output, gen_output, target, include_perceptual):\n",
    "    real_labels = torch.ones_like(disc_generated_output)\n",
    "    gan_loss = loss_object(disc_generated_output, real_labels)\n",
    "    l1 = l1_loss_fn(gen_output, target)\n",
    "\n",
    "    if include_perceptual:\n",
    "        perc = perceptual_loss(target, gen_output)\n",
    "        total_loss = gan_loss + (LAMBDA_L1 * l1) + (LAMBDA_PERC * perc)\n",
    "        return total_loss, gan_loss, l1, perc\n",
    "    else:\n",
    "        total_loss = gan_loss + (LAMBDA_L1 * l1)\n",
    "        return total_loss, gan_loss, l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d7477b6-2059-4bed-b53a-7ed8e9d95288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_labels = torch.ones_like(disc_real_output)\n",
    "    fake_labels = torch.zeros_like(disc_generated_output)\n",
    "\n",
    "    real_loss = loss_object(disc_real_output, real_labels)\n",
    "    fake_loss = loss_object(disc_generated_output, fake_labels)\n",
    "\n",
    "    total_disc_loss = real_loss + fake_loss\n",
    "    return total_disc_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95c039f0-c8bb-48b4-8d5f-1c44da4beb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_LR = 0.0002\n",
    "DISC_LR = 0.0002\n",
    "DAE_LR = 0.0001\n",
    "BETA_1 = 0.5\n",
    "BETA_2 = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdd5ead1-b5b0-4e4a-830c-7fbd29781e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = Adam(generator.parameters(), lr=GEN_LR, betas=(BETA_1, BETA_2))\n",
    "discriminator_optimizer = Adam(discriminator.parameters(), lr=DISC_LR, betas=(BETA_1, BETA_2))\n",
    "dae_optimizer = Adam(dae.parameters(),lr = DAE_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8e0e047-7a9a-4a8d-abd3-16ad1d90153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_loss(gen_output, target):\n",
    "    mse_loss = F.mse_loss(gen_output, target)  # Mean Squared Error\n",
    "    return mse_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6641164-16a9-4f39-88f8-fa3d3088ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(grayscale, color, include_perceptual=False):\n",
    "    grayscale = grayscale.to(device)\n",
    "    color = color.to(device)\n",
    "    \n",
    "    # ----- Generator & DAE Forward Pass -----\n",
    "    fake_color = generator(grayscale)\n",
    "    \n",
    "    # ----- Train DAE -----\n",
    "    dae_optimizer.zero_grad()\n",
    "    denoised_fake_color = dae(fake_color.detach())  # Detach to break computational graph\n",
    "    dae_loss = nn.MSELoss()(denoised_fake_color, color)\n",
    "    dae_loss.backward()\n",
    "    dae_optimizer.step()\n",
    "    \n",
    "    # ----- Train Discriminator -----\n",
    "    discriminator_optimizer.zero_grad()\n",
    "    # Recompute denoised_fake_color after DAE has been updated\n",
    "    with torch.no_grad():\n",
    "        denoised_fake_color = dae(fake_color.detach())\n",
    "    disc_real_output = discriminator(grayscale, color)\n",
    "    disc_fake_output = discriminator(grayscale, denoised_fake_color.detach())\n",
    "    disc_loss = discriminator_loss(disc_real_output, disc_fake_output)\n",
    "    disc_loss.backward()\n",
    "    discriminator_optimizer.step()\n",
    "    \n",
    "    # ----- Train Generator -----\n",
    "    generator_optimizer.zero_grad()\n",
    "    # We need fresh outputs since we've updated the other networks\n",
    "    fake_color = generator(grayscale)\n",
    "    denoised_fake_color = dae(fake_color)  # No detach here as we need gradients to flow back\n",
    "    disc_fake_output = discriminator(grayscale, denoised_fake_color)  # No detach for the same reason\n",
    "    \n",
    "    if include_perceptual:\n",
    "        gen_loss, adv_loss, l1_loss, perc_loss = generator_loss(\n",
    "            disc_fake_output, denoised_fake_color, color, include_perceptual\n",
    "        )\n",
    "    else:\n",
    "        gen_loss, adv_loss, l1_loss = generator_loss(\n",
    "            disc_fake_output, denoised_fake_color, color, include_perceptual\n",
    "        )\n",
    "    \n",
    "    gen_loss.backward()\n",
    "    generator_optimizer.step()\n",
    "    \n",
    "    if include_perceptual:\n",
    "        return gen_loss, adv_loss, l1_loss, perc_loss, dae_loss, disc_loss\n",
    "    else:\n",
    "        return gen_loss, adv_loss, l1_loss, dae_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cf101cb-2cb6-465a-8d95-98cd5bd0e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "loss_history = []\n",
    "include_metrics = True\n",
    "include_perceptual = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f655207d-21ab-4b92-a913-7b0ad9b50f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = 'checkpoints_Pix2Pix_DAE'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e47ffe92-8ec3-452d-8cb0-75bb310da99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Gen: 35.1298 | Adv: 4.6154 | L1: 0.2924 | Perc: 1.2770 | DAE: 0.1516 | Disc: 0.1660 | Time: 1m 22s\n",
      "Epoch 2/50 | Gen: 35.2246 | Adv: 6.2156 | L1: 0.2774 | Perc: 1.2739 | DAE: 0.1383 | Disc: 0.0430 | Time: 1m 22s\n",
      "Epoch 3/50 | Gen: 33.9300 | Adv: 5.2738 | L1: 0.2738 | Perc: 1.2774 | DAE: 0.1344 | Disc: 0.2183 | Time: 1m 21s\n",
      "Epoch 4/50 | Gen: 34.8596 | Adv: 6.9831 | L1: 0.2660 | Perc: 1.2766 | DAE: 0.1283 | Disc: 0.0316 | Time: 1m 21s\n",
      "Epoch 5/50 | Gen: 34.6036 | Adv: 7.2379 | L1: 0.2609 | Perc: 1.2763 | DAE: 0.1242 | Disc: 0.0357 | Time: 1m 21s\n",
      "Epoch 6/50 | Gen: 34.7759 | Adv: 7.8044 | L1: 0.2569 | Perc: 1.2817 | DAE: 0.1208 | Disc: 0.0332 | Time: 1m 21s\n",
      "Epoch 7/50 | Gen: 33.7708 | Adv: 7.5919 | L1: 0.2489 | Perc: 1.2846 | DAE: 0.1144 | Disc: 0.0540 | Time: 1m 21s\n",
      "Epoch 8/50 | Gen: 34.1552 | Adv: 8.8670 | L1: 0.2400 | Perc: 1.2862 | DAE: 0.1078 | Disc: 0.0021 | Time: 1m 21s\n",
      "Epoch 9/50 | Gen: 31.4056 | Adv: 7.2695 | L1: 0.2285 | Perc: 1.2894 | DAE: 0.0986 | Disc: 0.0735 | Time: 1m 21s\n",
      "Epoch 10/50 | Gen: 31.0428 | Adv: 7.9621 | L1: 0.2179 | Perc: 1.2926 | DAE: 0.0912 | Disc: 0.0162 | Time: 1m 21s\n",
      "-------------------------------------\n",
      "SSIM: -0.0187 | PSNR: 4.70 dB | MSE: 0.3458 | RMSE: 0.5851\n",
      "-------------------------------------\n",
      "Epoch 11/50 | Gen: 32.7182 | Adv: 9.0611 | L1: 0.2237 | Perc: 1.2898 | DAE: 0.0959 | Disc: 0.0008 | Time: 1m 21s\n",
      "Epoch 12/50 | Gen: 31.3236 | Adv: 9.6038 | L1: 0.2043 | Perc: 1.2928 | DAE: 0.0817 | Disc: 0.0003 | Time: 1m 21s\n",
      "Epoch 13/50 | Gen: 29.5828 | Adv: 8.6949 | L1: 0.1959 | Perc: 1.2971 | DAE: 0.0762 | Disc: 0.0457 | Time: 1m 21s\n",
      "Epoch 14/50 | Gen: 29.5763 | Adv: 9.2554 | L1: 0.1902 | Perc: 1.2981 | DAE: 0.0719 | Disc: 0.0009 | Time: 1m 21s\n",
      "Epoch 15/50 | Gen: 29.4178 | Adv: 9.9716 | L1: 0.1815 | Perc: 1.3003 | DAE: 0.0658 | Disc: 0.0003 | Time: 1m 21s\n",
      "Epoch 16/50 | Gen: 28.5123 | Adv: 9.3448 | L1: 0.1787 | Perc: 1.3022 | DAE: 0.0640 | Disc: 0.0717 | Time: 1m 21s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_perceptual:\n\u001b[0;32m      9\u001b[0m     gen_loss, adv_loss, l1_loss, perc_loss, dae_loss, disc_loss \u001b[38;5;241m=\u001b[39m train_step(grayscale, color, include_perceptual)\n\u001b[1;32m---> 10\u001b[0m     perc_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m perc_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     gen_loss, adv_loss, l1_loss, dae_loss, disc_loss \u001b[38;5;241m=\u001b[39m train_step(grayscale, color, include_perceptual)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    # Accumulators\n",
    "    gen_loss_total, adv_loss_total, l1_loss_total, perc_loss_total, dae_loss_total, disc_loss_total = 0, 0, 0, 0, 0, 0\n",
    "    num_batches = len(subset_loader)\n",
    "\n",
    "    for grayscale, color in subset_loader:\n",
    "        if include_perceptual:\n",
    "            gen_loss, adv_loss, l1_loss, perc_loss, dae_loss, disc_loss = train_step(grayscale, color, include_perceptual)\n",
    "            perc_loss_total += perc_loss.item()\n",
    "        else:\n",
    "            gen_loss, adv_loss, l1_loss, dae_loss, disc_loss = train_step(grayscale, color, include_perceptual)\n",
    "\n",
    "        # Accumulate losses\n",
    "        gen_loss_total += gen_loss.item()\n",
    "        adv_loss_total += adv_loss.item()\n",
    "        l1_loss_total += l1_loss.item()\n",
    "        dae_loss_total += dae_loss.item()\n",
    "        disc_loss_total += disc_loss.item()\n",
    "\n",
    "    # Time tracking\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    minutes = elapsed // 60\n",
    "    seconds = elapsed % 60\n",
    "\n",
    "    # Print logs\n",
    "    if include_perceptual:\n",
    "        print(f\"Epoch {epoch}/50 | Gen: {gen_loss_total/num_batches:.4f} | Adv: {adv_loss_total/num_batches:.4f} | \"\n",
    "              f\"L1: {l1_loss_total/num_batches:.4f} | Perc: {perc_loss_total/num_batches:.4f} | \"\n",
    "              f\"DAE: {dae_loss_total/num_batches:.4f} | Disc: {disc_loss_total/num_batches:.4f} | \"\n",
    "              f\"Time: {int(minutes)}m {int(seconds)}s\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch}/50 | Gen: {gen_loss_total/num_batches:.4f} | Adv: {adv_loss_total/num_batches:.4f} | \"\n",
    "              f\"L1: {l1_loss_total/num_batches:.4f} | DAE: {dae_loss_total/num_batches:.4f} | \"\n",
    "              f\"Disc: {disc_loss_total/num_batches:.4f} | Time: {int(minutes)}m {int(seconds)}s\")\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    if (epoch % 10) == 0 and include_metrics:\n",
    "        print(\"-------------------------------------\")\n",
    "        evaluate_model(subset_loader, generator, device)\n",
    "        print(\"-------------------------------------\")\n",
    "        # Save models\n",
    "        torch.save(generator.state_dict(), f\"Models/DAE_GEN/generator_epoch_{epoch}.pth\")\n",
    "        torch.save(discriminator.state_dict(), f\"Models/DAE_DISC/discriminator_epoch_{epoch}.pth\")\n",
    "visualize_colorization_results(generator,plot_loader,num_images = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddfb0cf-5596-4bc5-9ac5-f526d6d99815",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(51, 101):\n",
    "    start_time = time.time()\n",
    "    # Accumulators\n",
    "    gen_loss_total, adv_loss_total, l1_loss_total, perc_loss_total, dae_loss_total, disc_loss_total = 0, 0, 0, 0, 0, 0\n",
    "    num_batches = len(subset_loader)\n",
    "\n",
    "    for grayscale, color in subset_loader:\n",
    "        if include_perceptual:\n",
    "            gen_loss, adv_loss, l1_loss, perc_loss, dae_loss, disc_loss = train_step(grayscale, color, include_perceptual)\n",
    "            perc_loss_total += perc_loss.item()\n",
    "        else:\n",
    "            gen_loss, adv_loss, l1_loss, dae_loss, disc_loss = train_step(grayscale, color, include_perceptual)\n",
    "\n",
    "        # Accumulate losses\n",
    "        gen_loss_total += gen_loss.item()\n",
    "        adv_loss_total += adv_loss.item()\n",
    "        l1_loss_total += l1_loss.item()\n",
    "        dae_loss_total += dae_loss.item()\n",
    "        disc_loss_total += disc_loss.item()\n",
    "\n",
    "    # Time tracking\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    minutes = elapsed // 60\n",
    "    seconds = elapsed % 60\n",
    "\n",
    "    # Print logs\n",
    "    if include_perceptual:\n",
    "        print(f\"Epoch {epoch}/100 | Gen: {gen_loss_total/num_batches:.4f} | Adv: {adv_loss_total/num_batches:.4f} | \"\n",
    "              f\"L1: {l1_loss_total/num_batches:.4f} | Perc: {perc_loss_total/num_batches:.4f} | \"\n",
    "              f\"DAE: {dae_loss_total/num_batches:.4f} | Disc: {disc_loss_total/num_batches:.4f} | \"\n",
    "              f\"Time: {int(minutes)}m {int(seconds)}s\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch}/100 | Gen: {gen_loss_total/num_batches:.4f} | Adv: {adv_loss_total/num_batches:.4f} | \"\n",
    "              f\"L1: {l1_loss_total/num_batches:.4f} | DAE: {dae_loss_total/num_batches:.4f} | \"\n",
    "              f\"Disc: {disc_loss_total/num_batches:.4f} | Time: {int(minutes)}m {int(seconds)}s\")\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    if (epoch % 10) == 0 and include_metrics:\n",
    "        print(\"-------------------------------------\")\n",
    "        evaluate_model(subset_loader, generator, device)\n",
    "        # \n",
    "        print(\"-------------------------------------\")\n",
    "        # Save models\n",
    "        torch.save(generator.state_dict(), f\"Models/DAE_GEN/generator_epoch_{epoch}.pth\")\n",
    "        torch.save(discriminator.state_dict(), f\"Models/DAE_DISC/discriminator_epoch_{epoch}.pth\")\n",
    "visualize_colorization_results(generator,plot_loader,num_images = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
