{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "150ff5f8-31c4-4914-8688-61ac192751e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from skimage.metrics import structural_similarity\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.models import vgg16, vgg19\n",
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n",
    "\n",
    "from LabDataset import PairedImageDataset \n",
    "from Pairing_Images import PairFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5027ed98-a502-4654-ab76-f91d9b7cae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (256,256,3)\n",
    "TARGET_SHAPE = (256,256,3)\n",
    "BATCH_SIZE = 1\n",
    "# Dataset Hyper Parameters\n",
    "subset = \"agri\"\n",
    "save_dataframe = \"True\"\n",
    "s1_image_path = \"Dataset/agri/s1/\"\n",
    "s2_image_path = \"Dataset/agri/s2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ae8d68a-672b-4e55-bf35-41636824e776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Instances = 4000\n",
      "Subset Instances (1000 max) = 1000\n",
      "Plot Instances (10 max) = 10\n"
     ]
    }
   ],
   "source": [
    "image_dataset = PairedImageDataset(s1_dir=s1_image_path,s2_dir=s2_image_path,subset_name=subset,save_dataframe=save_dataframe,image_size=IMG_SHAPE[0])\n",
    "\n",
    "# 1. Full Dataset Loader\n",
    "dataloader = DataLoader(image_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f\"Total Instances = {len(image_dataset)}\")\n",
    "\n",
    "# 2. Subset Dataset (first 1000 samples)\n",
    "subset_indices = list(range(min(1000, len(image_dataset))))\n",
    "subset_dataset = Subset(image_dataset, subset_indices)\n",
    "subset_loader = DataLoader(subset_dataset, batch_size=1, shuffle=False)\n",
    "print(f\"Subset Instances (1000 max) = {len(subset_dataset)}\")\n",
    "\n",
    "# 3. Plot Dataset (first 10 samples)\n",
    "plot_indices = list(range(min(10, len(image_dataset))))\n",
    "plot_dataset = Subset(image_dataset, plot_indices)\n",
    "plot_loader = DataLoader(plot_dataset, batch_size=1, shuffle=False)\n",
    "print(f\"Plot Instances (10 max) = {len(plot_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cf851fc-0dc7-4127-a758-d76881ae9e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256]) torch.Size([2, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i,j in subset_dataset:\n",
    "    print(i.shape,j.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "762791c6-d141-4beb-9a1a-e783291254dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNetBlockDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, apply_batchnorm=True):\n",
    "        super(UNetBlockDown, self).__init__()\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "        if apply_batchnorm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class UNetBlockUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, apply_dropout=False):\n",
    "        super(UNetBlockUp, self).__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        if apply_dropout:\n",
    "            layers.append(nn.Dropout(0.5))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.block(x)\n",
    "        x = torch.cat([x, skip_input], dim=1)\n",
    "        return x\n",
    "\n",
    "class UNetGeneratorLAB(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=2):  # L -> ab\n",
    "        super(UNetGeneratorLAB, self).__init__()\n",
    "        self.down1 = UNetBlockDown(in_channels, 64, apply_batchnorm=False)\n",
    "        self.down2 = UNetBlockDown(64, 128)\n",
    "        self.down3 = UNetBlockDown(128, 256)\n",
    "        self.down4 = UNetBlockDown(256, 512)\n",
    "        self.down5 = UNetBlockDown(512, 512)\n",
    "        self.down6 = UNetBlockDown(512, 512)\n",
    "        self.down7 = UNetBlockDown(512, 512)\n",
    "        self.down8 = UNetBlockDown(512, 512, apply_batchnorm=False)\n",
    "\n",
    "        self.up1 = UNetBlockUp(512, 512, apply_dropout=True)\n",
    "        self.up2 = UNetBlockUp(1024, 512, apply_dropout=True)\n",
    "        self.up3 = UNetBlockUp(1024, 512, apply_dropout=True)\n",
    "        self.up4 = UNetBlockUp(1024, 512)\n",
    "        self.up5 = UNetBlockUp(1024, 256)\n",
    "        self.up6 = UNetBlockUp(512, 128)\n",
    "        self.up7 = UNetBlockUp(256, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()  # Output ab channels in [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "\n",
    "        return self.final(u7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba869f1a-4b73-479d-acbd-3552e37db9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchGAN Discriminator for LAB image colorization.\n",
    "    - input_image: L channel (1 channel)\n",
    "    - target_image: ab channels (2 channels)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=3):  # L + ab = 1 + 2\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            self._block(in_channels, 64, norm=False),  # [L|ab] = 3 channels\n",
    "            self._block(64, 128),\n",
    "            self._block(128, 256),\n",
    "            nn.ZeroPad2d(1),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ZeroPad2d(1),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, norm=True):\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "        if norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input_l, target_ab):\n",
    "        \"\"\"\n",
    "        input_l: Tensor [B, 1, H, W] — grayscale input (L channel)\n",
    "        target_ab: Tensor [B, 2, H, W] — real or generated color (ab channels)\n",
    "        \"\"\"\n",
    "        x = torch.cat([input_l, target_ab], dim=1)  # concat on channel dimension → [B, 3, H, W]\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98a88a9f-7055-4af4-aa36-81acfcba13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = UNetGeneratorLAB(in_channels=1, out_channels=2).to(device)\n",
    "discriminator = PatchDiscriminator(in_channels=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39ed6958-9d29-4999-a0a1-3a7fac87dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA_L1 = 100\n",
    "LAMBDA_PERC = 0.01\n",
    "loss_object = nn.BCEWithLogitsLoss()\n",
    "l1_loss_fn = nn.L1Loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eac61d10-1792-455a-accc-5c25e4515220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(disc_generated_output, gen_output, target, include_perceptual):\n",
    "    real_labels = torch.ones_like(disc_generated_output)\n",
    "    gan_loss = loss_object(disc_generated_output, real_labels)\n",
    "    l1 = l1_loss_fn(gen_output, target)\n",
    "\n",
    "    if include_perceptual:\n",
    "        perc = perceptual_loss(target, gen_output)\n",
    "        total_loss = gan_loss + (LAMBDA_L1 * l1) + (LAMBDA_PERC * perc)\n",
    "        return total_loss, gan_loss, l1, perc\n",
    "    else:\n",
    "        total_loss = gan_loss + (LAMBDA_L1 * l1)\n",
    "        return total_loss, gan_loss, l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e80f00a3-a762-4ea7-837b-e8dbe3b8c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_labels = torch.ones_like(disc_real_output)\n",
    "    fake_labels = torch.zeros_like(disc_generated_output)\n",
    "\n",
    "    real_loss = loss_object(disc_real_output, real_labels)\n",
    "    fake_loss = loss_object(disc_generated_output, fake_labels)\n",
    "\n",
    "    total_disc_loss = real_loss + fake_loss\n",
    "    return total_disc_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0d94642-291f-4d86-8119-5fda3c34aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_LR = 0.0002\n",
    "DISC_LR = 0.0002\n",
    "BETA_1 = 0.5\n",
    "BETA_2 = 0.999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3c5cd15-aa7a-448b-b297-db715f190b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = Adam(generator.parameters(), lr=GEN_LR, betas=(BETA_1, BETA_2))\n",
    "discriminator_optimizer = Adam(discriminator.parameters(), lr=DISC_LR, betas=(BETA_1, BETA_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df329c3f-7072-46fa-bfa3-ec9364f16a90",
   "metadata": {},
   "source": [
    "# Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3718695b-aa71-42e8-a921-a1064bb8747d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
